{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluator.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fPFXjAVFIKnh","executionInfo":{"status":"ok","timestamp":1648492034778,"user_tz":-120,"elapsed":7655,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"outputs":[],"source":["import argparse\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import pickle\n","import platform\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"uGxnwhvlwMiI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648492051777,"user_tz":-120,"elapsed":17027,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}},"outputId":"0c37c01b-4fb7-46e5-e57f-35dcfcad7e02"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/vita-epfl/DLAV-2022.git\n","path = os.getcwd() + '/DLAV-2022/homeworks/hw2/test_batch'"],"metadata":{"id":"SwxcJW9wI9fp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648492055147,"user_tz":-120,"elapsed":3392,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}},"outputId":"3f53bfe8-c88f-4e44-ef21-6a3c28dd86ef"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DLAV-2022'...\n","remote: Enumerating objects: 83, done.\u001b[K\n","remote: Counting objects: 100% (83/83), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 83 (delta 31), reused 60 (delta 16), pack-reused 0\u001b[K\n","Unpacking objects: 100% (83/83), done.\n"]}]},{"cell_type":"code","source":["# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n","### Path to Model Weights \n","softmax_weights = \"drive/MyDrive/Colab Notebooks/tp_2/softmax_weights.pkl\"\n","pytorch_weights = \"drive/MyDrive/Colab Notebooks/tp_2/linearClassifier_pytorch.ckpt\""],"metadata":{"id":"pZXQTJIKJE_S","executionInfo":{"status":"ok","timestamp":1648492055619,"user_tz":-120,"elapsed":486,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**TODO:** Copy your code from the Softmax Notebook to their corresponding function"],"metadata":{"id":"mE6psT_aVPHv"}},{"cell_type":"code","source":["\n","def softmax_loss_vectorized(W, X, y):\n","    \"\"\"\n","  Softmax loss function, vectorized version.\n","  Inputs and outputs are the same as softmax_loss_naive.\n","  \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    #############################################################################\n","    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n","    # Store the loss in loss and the gradient in dW. If you are not careful     #\n","    # here, it is easy to run into numeric instability. Don't forget the        #\n","    # regularization!                                                           #\n","    #############################################################################\n","    \n","    # array dimensions\n","    N = np.size(X, axis=0)\n","\n","    # Score for some W (linear): s = f(x,W) = X * W # [N x C] = [N x D] x [D x C]\n","    s = np.dot(X, W)                                # [N x C] = [N x D] x [D x C]\n","    s = s-np.max(s, axis=1, keepdims=True)          # normalization to avoid numerical instability during division by large exponential term\n","    exp_s = np.exp(s)                               # [N x C]\n","\n","    # Probability that sample i within [0 N-1] belongs to class k within [0 C-1]: P(Y=k|X=x_i)\n","    P = exp_s/np.sum(exp_s, axis=1, keepdims=True)  # [N x C] = [N x C] / [N x 1]\n","    # or (exp_s.T/np.sum(exp_s, axis=1).T).T\n","\n","    # Loss Function\n","    # Maximum Log-Likelyhood (correct classification): L_i = -log(P(Y=y_i|X=x_i))\n","    L_i = -np.log(P[np.arange(N),y])                # [N x 1]\n","    loss = np.sum(L_i)/N                            # [1 x 1]\n","    \n","    # Gradient\n","    # ds = dL_i/ds_j = P - (y_i==j)\n","    ds = P                                          # [N x C]\n","    ds[np.arange(N),y] -= 1                         # [N x C]\n","\n","    # dW = dL/dW_j = ds_j/dW_j * dL/ds_j = ds_j/dW_j * 1/N * dL_i/ds_j = X^T * 1/N * (P - (y_i==j))\n","    dW = np.dot(np.transpose(X), ds) / N            # [D x C] = [D x N] x [N x C] / [1 x 1]\n","\n","    #############################################################################\n","    #                          END OF YOUR CODE                                 #\n","    #############################################################################\n","    \n","    return loss, dW\n","\n","class LinearClassifier(object):\n","\n","    def __init__(self):\n","        self.W = None\n","\n","\n","    def train(self, X, y, learning_rate=1e-3, num_iters=30000,\n","                batch_size=200, verbose=False):\n","        \"\"\"\n","        Train this linear classifier using stochastic gradient descent.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","          training samples each of dimension D.\n","        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n","          means that X[i] has label 0 <= c < C for C classes.\n","        - learning_rate: (float) learning rate for optimization.\n","        - num_iters: (integer) number of steps to take when optimizing\n","        - batch_size: (integer) number of training examples to use at each step.\n","        - verbose: (boolean) If true, print progress during optimization.\n","\n","        Outputs:\n","        A list containing the value of the loss function at each training iteration.\n","        \"\"\"\n","        \n","        num_train, dim = X.shape\n","        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","        \n","        if self.W is None:\n","            # lazily initialize W\n","            self.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","        # Run stochastic gradient descent to optimize W\n","        loss_history = []\n","        for it in range(num_iters):\n","            X_batch = None\n","            y_batch = None\n","\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Sample batch_size elements from the training data and their           #\n","            # corresponding labels to use in this round of gradient descent.        #\n","            # Store the data in X_batch and their corresponding labels in           #\n","            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n","            # and y_batch should have shape (batch_size,)                           #\n","            #                                                                       #\n","            # Hint: Use np.random.choice to generate indices. Sampling with         #\n","            # replacement is faster than sampling without replacement.              #\n","            #########################################################################\n","            \n","            indices = np.random.choice(np.arange(num_train), size=batch_size)\n","            X_batch = X[indices,:]\n","            y_batch = y[indices]\n","            \n","            #########################################################################\n","            #                       END OF YOUR CODE                                #\n","            #########################################################################\n","\n","            # evaluate loss and gradient\n","            loss, grad = self.loss(X_batch, y_batch)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Update the weights using the gradient and the learning rate.          #\n","            #########################################################################\n","            \n","            self.W -= learning_rate*grad\n","            \n","            #########################################################################\n","            #                       END OF YOUR CODE                                #\n","            #########################################################################\n","\n","            if verbose and it % 100 == 0:\n","                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","\n","        return loss_history\n","    \n","\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Use the trained weights of this linear classifier to predict labels for\n","        data points.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","          training samples each of dimension D.\n","\n","        Returns:\n","        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","          array of length N, and each element is an integer giving the predicted\n","          class.\n","        \"\"\"\n","\n","        ###########################################################################\n","        # TODO:                                                                   #\n","        # Implement this method. Store the predicted labels in y_pred.            #\n","        ###########################################################################\n","        \n","        s = np.dot(X, self.W)                           # [N x C] = [N x D] x [D x C]\n","        s = s-np.max(s, axis=1, keepdims=True)          # normalization to avoid numerical instability during division by large exponential term\n","        exp_s = np.exp(s)                               # [N x C]\n","\n","        # Probability that sample i within [0 N-1] belongs to class k within [0 C-1]: P(Y=k|X=x_i)\n","        P = exp_s/np.sum(exp_s, axis=1, keepdims=True)  # [N x C] = [N x C] / [N x 1]\n","\n","        # Predicted Class Label\n","        y_pred = np.argmax(P, axis=1)\n","\n","        ###########################################################################\n","        #                           END OF YOUR CODE                              #\n","        ###########################################################################\n","        return y_pred\n","\n","    def loss(self, X_batch, y_batch):\n","        \"\"\"\n","        Compute the loss function and its derivative. \n","        Subclasses will override this.\n","\n","        Inputs:\n","        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n","          data points; each point has dimension D.\n","        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n","\n","\n","        Returns: A tuple containing:\n","        - loss as a single float\n","        - gradient with respect to self.W; an array of the same shape as W\n","        \n","         e = y_batch - np.dot(X_batch, self.W) \n","        \n","        loss = np.dot(e.T, e)\n","        grad = -np.dot(x_batch.T,e) / x_batch.shape[0]\n","  \n","        return loss, grad\n","\n","        \"\"\"\n","\n","        pass\n","        \n","\n","\n","class Softmax(LinearClassifier):\n","    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n","\n","    def loss(self, X_batch, y_batch):\n","        return softmax_loss_vectorized(self.W, X_batch, y_batch)"],"metadata":{"id":"gHnLX6-oIkWm","executionInfo":{"status":"ok","timestamp":1648492055622,"user_tz":-120,"elapsed":26,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**TODO:** Copy the model you created from the Pytorch Notebook"],"metadata":{"id":"6chaH4G-Vfms"}},{"cell_type":"code","source":["class Net(torch.nn.Module):\n","    def __init__(self, n_feature, n_hidden, n_output):\n","        super(Net, self).__init__()\n","        \n","        # define fully connected layers\n","        self.fc1 = torch.nn.Linear(in_features=n_feature, out_features=n_hidden)\n","        self.fc2 = torch.nn.Linear(in_features=n_hidden, out_features=n_output)\n","\n","\n","    def forward(self, x):\n","        \n","        # x = [40 x n_feature]\n","        x = self.fc1(x)   # [40 x n_hidden]\n","        x = F.relu(x)     # [40 x n_hidden]\n","        x = self.fc2(x)   # [40 x n_output]\n","\n","        return x"],"metadata":{"id":"mSTfKTHEJBhy","executionInfo":{"status":"ok","timestamp":1648492055624,"user_tz":-120,"elapsed":25,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n","\n"],"metadata":{"id":"_UUbNTUAVsos"}},{"cell_type":"code","source":["def predict_usingPytorch(X):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # - Create your model                                                   #\n","    # - Load your saved model                                               #\n","    # - Do the operation required to get the predictions                    #\n","    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n","    #########################################################################\n","    nb_features=32*32*3   # image size * color channels\n","\n","    net = Net(n_feature=nb_features, n_hidden=2048, n_output=10)     # define the network\n","    checkpoint = torch.load(\"drive/MyDrive/Colab Notebooks/tp_2/linearClassifier_pytorch.ckpt\")\n","    net.load_state_dict(checkpoint) # Load the best computed parameters\n","\n","    print(net)  # net architecture\n","    \n","    outputs = net(X.view(-1,nb_features))  # don't use net.forward(X)  # make X compatible with W\n","    y_pred = torch.argmax(F.softmax(outputs).data, 1)\n","\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n","    return y_pred.numpy()\n","\n","def predict_usingSoftmax(X):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # - Load your saved model into the weights of Softmax                   #\n","    # - Do the operation required to get the predictions                    #\n","    # - Return predictions in a numpy array                                 #\n","    #########################################################################\n","    \n","    with open('drive/MyDrive/Colab Notebooks/tp_2/softmax_weights.pkl', 'rb') as f:\n","      W = pickle.load(f)\n","    new_softmax = Softmax()\n","    new_softmax.W = W.copy()\n","\n","    y_pred = new_softmax.predict(X)\n","\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n","    return y_pred"],"metadata":{"id":"bEKafMuaI4By","executionInfo":{"status":"ok","timestamp":1648492055627,"user_tz":-120,"elapsed":26,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["This method loads the test dataset to evaluate the model."],"metadata":{"id":"q8dM8fj39OBP"}},{"cell_type":"code","source":["## Read DATA\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","test_filename = path\n","X,Y = load_CIFAR_batch(test_filename)"],"metadata":{"id":"400u4eZNJAZq","executionInfo":{"status":"ok","timestamp":1648492061678,"user_tz":-120,"elapsed":339,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here. "],"metadata":{"id":"AJ3mBYnx9TIe"}},{"cell_type":"code","source":["## Data Manipulation\n","\n","mean = np.array([0.4914, 0.4822, 0.4465])\n","std = np.array([0.2023, 0.1994, 0.2010])\n","X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n","\n","X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n","X_softmax = np.reshape(X, (X.shape[0], -1))\n","X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"],"metadata":{"id":"IEmU5KnwJPBY","executionInfo":{"status":"ok","timestamp":1648492064768,"user_tz":-120,"elapsed":540,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n","\n","---\n","\n"],"metadata":{"id":"O2nQbKPL9c3G"}},{"cell_type":"code","source":["## Run Prediction\n","prediction_pytorch = predict_usingPytorch(X_pytorch)\n","prediction_softmax = predict_usingSoftmax(X_softmax)\n","\n","## Run Evaluation\n","acc_softmax = sum(prediction_softmax == Y)/len(X)\n","acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n","print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))"],"metadata":{"id":"VKFPhm1wJjDv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648492068895,"user_tz":-120,"elapsed":2414,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}},"outputId":"a1777a62-c6e9-40cc-e5d7-cc5055f8f34d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (fc1): Linear(in_features=3072, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=10, bias=True)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["Softmax= 0.369000 ... Pytorch= 0.635300\n"]}]}]}