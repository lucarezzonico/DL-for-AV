{"cells":[{"cell_type":"markdown","metadata":{"id":"VwB8YxH7_Bej"},"source":["# Project - Deep Learning for Autonomous Vehicules\n","### Group 11 :  Luca Rezzonico, Aya Zghari, Arwen Giraud"]},{"cell_type":"markdown","source":["## Milestone 1 - Detection"],"metadata":{"id":"TGsSgx4zqvX7"}},{"cell_type":"markdown","source":["### Initialization"],"metadata":{"id":"Hix6yy5-rhdl"}},{"cell_type":"markdown","source":["#### Required instalations"],"metadata":{"id":"ir_UAK2_qz7s"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15509,"status":"ok","timestamp":1651500936277,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"},"user_tz":-120},"id":"xPd5pBqGoPmN","outputId":"09f649c8-3c99-43fd-86b9-33eb3f75146e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n","\u001b[K     |████████████████████████████████| 32.7 MB 136 kB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.2.0)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.9.1\n","Collecting opencv-python-headless<4.3\n","  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n","\u001b[K     |████████████████████████████████| 21.6 MB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.6)\n","Installing collected packages: opencv-python-headless\n","Successfully installed opencv-python-headless-4.2.0.34\n"]}],"source":["!pip install mediapipe opencv-python\n","!pip install \"opencv-python-headless<4.3\""]},{"cell_type":"markdown","source":["#### Add dependencies"],"metadata":{"id":"bJrDI9K6q8HW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CojALqcQ_BxI"},"outputs":[],"source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","\n","import PIL\n","import io\n","import html\n","import cv2\n","import time\n","import numpy as np\n","import mediapipe as mp\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","source":["\n","\n","```\n","# Ce texte est au format code\n","```\n","\n","####Import mediapipe library for joint detection"],"metadata":{"id":"6040QxxJrSdB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHDPDIJMogXM"},"outputs":[],"source":["mp_holistic = mp.solutions.holistic"]},{"cell_type":"markdown","source":["### Constants"],"metadata":{"id":"aHtpTUGd51Jk"}},{"cell_type":"code","source":["# Store the indexes of the tips landmarks of each finger of a hand in a list.\n","fingers_tips_ids = [mp_holistic.HandLandmark.INDEX_FINGER_TIP, mp_holistic.HandLandmark.MIDDLE_FINGER_TIP,\n","                    mp_holistic.HandLandmark.RING_FINGER_TIP, mp_holistic.HandLandmark.PINKY_TIP]"],"metadata":{"id":"PgX411Xr53z8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function definition"],"metadata":{"id":"g2N84LV4r5-o"}},{"cell_type":"markdown","source":["Setting Up Darknet for YOLOv4\n"],"metadata":{"id":"sqAeMi4P0Wb9"}},{"cell_type":"code","source":["# clone darknet repo\n","!git clone https://github.com/AlexeyAB/darknet"],"metadata":{"id":"W27eWMm20U1-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651500939539,"user_tz":-120,"elapsed":2924,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}},"outputId":"c2d304f6-634a-4901-9f83-1ac71df19d89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'darknet'...\n","remote: Enumerating objects: 15412, done.\u001b[K\n","remote: Total 15412 (delta 0), reused 0 (delta 0), pack-reused 15412\u001b[K\n","Receiving objects: 100% (15412/15412), 14.02 MiB | 19.51 MiB/s, done.\n","Resolving deltas: 100% (10356/10356), done.\n"]}]},{"cell_type":"code","source":["# change makefile to have GPU, OPENCV and LIBSO enabled\n","%cd darknet\n","!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n","!sed -i 's/GPU=0/GPU=1/' Makefile\n","!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n","!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n","!sed -i 's/LIBSO=0/LIBSO=1/' Makefile"],"metadata":{"id":"qSy5XkVJ0hFr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651500940436,"user_tz":-120,"elapsed":911,"user":{"displayName":"Luca Rezzonico","userId":"13659644689464206961"}},"outputId":"d0dcc5db-b8be-49aa-d6c9-2015b6d86179"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/darknet\n"]}]},{"cell_type":"code","source":["# make darknet (builds darknet so that you can then use the darknet.py file and have its dependencies)\n","!make"],"metadata":{"id":"tfPhxVRo0jzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get bthe scaled yolov4 weights file that is pre-trained to detect 80 classes (objects) from shared google drive\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V3vsIaxAlGWvK4Aar9bAiK5U0QFttKwq' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1V3vsIaxAlGWvK4Aar9bAiK5U0QFttKwq\" -O yolov4-csp.weights && rm -rf /tmp/cookies.txt"],"metadata":{"id":"Juxnmbfs0nyN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import darknet functions to perform object detections\n","from darknet import *\n","# load in our YOLOv4 architecture network\n","network, class_names, class_colors = load_network(\"cfg/yolov4-csp.cfg\", \"cfg/coco.data\", \"yolov4-csp.weights\")\n","width = network_width(network)\n","height = network_height(network)\n","\n","# darknet helper function to run detection on image\n","def darknet_helper(img, width, height):\n","  darknet_image = make_image(width, height, 3)\n","  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  img_resized = cv2.resize(img_rgb, (width, height),\n","                              interpolation=cv2.INTER_LINEAR)\n","\n","  # get image ratios to convert bounding boxes to proper size\n","  img_height, img_width, _ = img.shape\n","  width_ratio = img_width/width\n","  height_ratio = img_height/height\n","\n","  # run model on darknet style image to get detections\n","  copy_image_from_bytes(darknet_image, img_resized.tobytes())\n","  detections = detect_image(network, class_names, darknet_image)\n","  free_image(darknet_image)\n","  return detections, width_ratio, height_ratio"],"metadata":{"id":"eD2pO-Wa0yHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Convertion functions\n","\n"],"metadata":{"id":"ff8CF-ETsEvM"}},{"cell_type":"markdown","source":["Function to convert JavaScript object into OpenCV image"],"metadata":{"id":"AbW1zVQNsmC6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BO_tg7nVpMU4"},"outputs":[],"source":["def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img"]},{"cell_type":"markdown","source":["function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream"],"metadata":{"id":"yjQamXHvsxid"}},{"cell_type":"code","source":["def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"VjN2vXbVsx5a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Video functions"],"metadata":{"id":"z4Ic6wwYs7aa"}},{"cell_type":"markdown","source":["JavaScript to properly create our live video stream using our webcam as input"],"metadata":{"id":"RNR3p5U8tEnt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkt8NaeepAfq"},"outputs":[],"source":["def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)"]},{"cell_type":"markdown","source":["Function to add the bbox to the live video"],"metadata":{"id":"AyyjnyaWtNmM"}},{"cell_type":"code","source":["def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"metadata":{"id":"fzi7Q_k0tNwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Take a photo and analyse the symbol on it"],"metadata":{"id":"7CImO5A09tMz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKWwICMcpRx2"},"outputs":[],"source":["def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","\n","  # get photo data\n","  data = eval_js('takePhoto({})'.format(quality))\n","\n","  with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n","      # get OpenCV format image\n","      img = js_to_image(data)\n","      #img = cv2.flip(img, 1)\n","\n","      bbox_array, fingers_statuses, image = process_img(img, holistic)\n","\n","      # Image back to BGR for rendering\n","      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","      # save image\n","      cv2.imwrite(filename, image)\n","\n","  return filename, fingers_statuses"]},{"cell_type":"markdown","metadata":{"id":"lG47wYar85fy"},"source":["#### Functions for symbole recognition"]},{"cell_type":"markdown","source":["This function will count the number of fingers up for each hand in the image."],"metadata":{"id":"p9q3bUzWt6iw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSueiY_V85ux"},"outputs":[],"source":["def countFingers(image, hand_landmarks, hand_label, fingers_statuses):\n","    '''\n","    Params:\n","        image:            The image of the hands on which the fingers counting is required to be performed.\n","        hand_landmarks:   The output of the hands landmarks detection performed on the image of the hands.\n","        hand_label:       'Left' or 'Right'\n","        fingers_statuses: A dictionary containing the status (i.e., open or close) of each finger of both hands.\n","    Returns:\n","        fingers_statuses: A dictionary containing the status (i.e., open or close) of each finger of both hands.\n","    '''    \n","    # Iterate over the indexes of the tips landmarks of each finger of the hand.\n","    for tip_index in fingers_tips_ids:\n","        \n","        # Retrieve the label (i.e., index, middle, etc.) of the finger on which we are iterating upon.\n","        finger_name = tip_index.name.split(\"_\")[0]\n","        \n","        # Check if the finger is up by comparing the y-coordinates of the tip and pip landmarks.\n","        if (hand_landmarks.landmark[tip_index].y < hand_landmarks.landmark[tip_index - 2].y):\n","            \n","            # Update the status of the finger in the dictionary to true.\n","            fingers_statuses[hand_label.upper()+\"_\"+finger_name] = True\n","    \n","    # Retrieve the y-coordinates of the tip and mcp landmarks of the thumb of the hand.\n","    thumb_tip_x = hand_landmarks.landmark[4].x\n","    thumb_mcp_x = hand_landmarks.landmark[2].x\n","    \n","    # Check if the thumb is up by comparing the hand label and the x-coordinates of the retrieved landmarks.\n","    if (hand_label=='Right' and (thumb_tip_x > thumb_mcp_x)) or (hand_label=='Left' and (thumb_tip_x < thumb_mcp_x)):\n","        \n","        # Update the status of the thumb in the dictionary to true.\n","        fingers_statuses[hand_label.upper()+\"_THUMB\"] = True\n","\n","    # Return the status of each finger\n","    return fingers_statuses"]},{"cell_type":"markdown","source":["Analyse image"],"metadata":{"id":"9Almuq84vLbT"}},{"cell_type":"code","source":["def process_img(img, holistic):\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # Convert an image from one color space to another => BGE to RGB\n","    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    w, h, c = image.shape\n","\n","    # Make Detections\n","    results = holistic.process(image)\n","\n","    # Initialize a dictionary to store the status (i.e., True for open and False for close) of each finger of both hands.\n","    fingers_statuses = {'RIGHT_THUMB': False, 'RIGHT_INDEX': False, 'RIGHT_MIDDLE': False, 'RIGHT_RING': False,\n","                        'RIGHT_PINKY': False, 'LEFT_THUMB': False, 'LEFT_INDEX': False, 'LEFT_MIDDLE': False,\n","                        'LEFT_RING': False, 'LEFT_PINKY': False}\n","\n","    # get face bounding box for overlay\n","    if results.right_hand_landmarks:\n","        x_min = round(results.right_hand_landmarks.landmark[5].x * h)\n","        x_max = round(results.right_hand_landmarks.landmark[17].x * h)\n","        y_min = round(results.right_hand_landmarks.landmark[12].y * w)\n","        y_max = round(results.right_hand_landmarks.landmark[0].y * w)\n","\n","        bbox_array = cv2.rectangle(bbox_array,(x_min, y_min),(x_max, y_max),(255,0,0),2)\n","\n","        fingers_statuses = countFingers(image, results.right_hand_landmarks, 'Right', fingers_statuses)\n","\n","    if results.left_hand_landmarks:\n","        x_min = round(results.left_hand_landmarks.landmark[5].x * h)\n","        x_max = round(results.left_hand_landmarks.landmark[17].x * h)\n","        y_min = round(results.left_hand_landmarks.landmark[12].y * w)\n","        y_max = round(results.left_hand_landmarks.landmark[0].y * w)\n","\n","        bbox_array = cv2.rectangle(bbox_array,(x_min, y_min),(x_max, y_max),(0, 0, 255),2)\n","\n","        fingers_statuses = countFingers(image, results.left_hand_landmarks, 'Left', fingers_statuses)\n","\n","    return bbox_array, fingers_statuses, image"],"metadata":{"id":"2M6NowDTvLlq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if the symbole shown is the correct one"],"metadata":{"id":"5sjMthfS6lEu"}},{"cell_type":"code","source":["def check_symbol():\n","  correct_symbol = True\n","  for hand_label in ['Right', 'Left']:\n","      for tip_index in fingers_tips_ids:\n","        # Retrieve the label (i.e., index, middle, etc.) of the finger on which we are iterating upon.\n","        finger_name = tip_index.name.split(\"_\")[0]\n","            \n","        # Compare the status of the fingers.\n","        if fingers_statuses[hand_label.upper()+\"_\"+finger_name] != model_statuses[hand_label.upper()+\"_\"+finger_name]:\n","            correct_symbol = False\n","\n","  return correct_symbol"],"metadata":{"id":"L5vFXtRX6lR4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Program"],"metadata":{"id":"1VEdVoY292RO"}},{"cell_type":"markdown","source":["\n","\n","```\n","# Ce texte est au format code\n","```\n","\n","#### Step 1 - Define symbol"],"metadata":{"id":"QiS9n3AK97dQ"}},{"cell_type":"markdown","source":["Take a photo in order to have the reference symbol"],"metadata":{"id":"AnQiNond-ltg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvxbNhWfOa58"},"outputs":[],"source":["try:\n","  filename, model_statuses = take_photo('photo.jpg')\n","  print('Saved to {}'.format(filename))\n","  \n","  # Show the image which was just taken.\n","  display(Image(filename))\n","except Exception as err:\n","  # Errors will be thrown if the user does not have a webcam or if they do not\n","  # grant the page permission to access it.\n","  print(str(err))"]},{"cell_type":"markdown","source":["Sanity check : Just to verify fingers were correctly identified\n","\n"],"metadata":{"id":"k6mVqEgI-Uxj"}},{"cell_type":"code","source":["print(model_statuses)"],"metadata":{"id":"CQT6LcyMaKiL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Step 2 - Detection during live stream"],"metadata":{"id":"bTWC1Rfg-eTc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjTYdTndojda"},"outputs":[],"source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","\n","with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n","    while True:\n","        js_reply = video_frame(label_html, bbox)\n","        if not js_reply:\n","            break\n","\n","        # convert JS response to OpenCV Image\n","        img = js_to_image(js_reply[\"img\"])\n","\n","\n","        # call our darknet helper on video frame\n","        detections, width_ratio, height_ratio = darknet_helper(frame, width, height)\n","\n","        # loop through detections and draw them on transparent overlay image\n","        for label, confidence, bbox in detections:\n","            left, top, right, bottom = bbox2points(bbox)\n","            left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n","            bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)\n","            bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(label, float(confidence)),\n","                                     (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, class_colors[label], 2)\n","\n","\n","        bbox_array, fingers_statuses, image = process_img(img, holistic)\n","\n","        correct_symbol = check_symbol()\n","\n","        if not correct_symbol:\n","          bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","        bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","        # convert overlay of bbox into bytes\n","        bbox_bytes = bbox_to_bytes(bbox_array)\n","        # update bbox so next frame gets new overlay\n","        bbox = bbox_bytes\n","\n","        # Image back to BGR for rendering\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Project.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}